<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Home on gzt does statistics</title>
    <link>http://gzt.github.io/</link>
    <description>Recent content in Home on gzt does statistics</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Mon, 12 Feb 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="http://gzt.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>About</title>
      <link>http://gzt.github.io/about/</link>
      <pubDate>Mon, 12 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>http://gzt.github.io/about/</guid>
      <description>I&amp;rsquo;m Geoff Thompson, a PhD candidate at the Department of Statistics at Iowa State University. I should be defending this year. My research interests have been in clustering and other classification problems as well as unsupervised or semisupervised learning. But, generally, a lot of aspects of going from large messy real-world data to the truth of what is going on. Some call it data science but I call it statistics.</description>
    </item>
    
    <item>
<<<<<<< HEAD
      <title>Software for Matrix Variate LDA and QDA</title>
      <link>http://gzt.github.io/post/software-for-matrix-variate-lda-and-qda/</link>
      <pubDate>Thu, 22 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>http://gzt.github.io/post/software-for-matrix-variate-lda-and-qda/</guid>
      <description>In the previous post, I had some rough notes on classification of matrix variate data. In the matrixdist package, I now have some functions for training a linear or quadratic classifier. The usage is pretty similar to the function MASS::lda() or MASS::qda(), however it requires the input as an array or list of matrices and the group variable provided as a vector (that is, it cannot handle data frames or the formula interface directly, which is reasonable, as there is no immediately clear way to make that work for a collection of matrices - anybody using it would have to roll their own solutions anyway).</description>
    </item>
    
    <item>
=======
>>>>>>> b3070be22caa29c6eb32a0cce927d4c72d8e5762
      <title>Notes on Discriminant Analysis for Matrix Variate Distributions</title>
      <link>http://gzt.github.io/post/notes-on-discriminant-analysis-for-matrix-variate-distributions/</link>
      <pubDate>Tue, 20 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>http://gzt.github.io/post/notes-on-discriminant-analysis-for-matrix-variate-distributions/</guid>
      <description>I have some brief notes for a discussion here so I’m posting them even though they’re a little incomplete because why not? Two-class classification for matrix variate normal distributions.
Expected Cost of Misclassification ECM is expected cost of misclassification. Suppose there are two populations, \(\pi_1\) and \(\pi_2\) with prior probabilities of belonging to these classes, \(p_1\) and \(p_2\). Define a function, \(c(1|2)\) as the cost of misclassifying a member of population \(\pi_2\) as a member of class \(1\) (and vice versa).</description>
    </item>
    
    <item>
      <title>Matrixdist News</title>
      <link>http://gzt.github.io/post/matrixdist-new/</link>
      <pubDate>Fri, 16 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>http://gzt.github.io/post/matrixdist-new/</guid>
      <description>I have just finished setting up handling for AR(1) and compound symmetry variance structures in my maximum likelihood estimation function for matrix variate normal distributions in matrixdist. This means I may submit it to CRAN soon (it’s currently available on github). An example:
<<<<<<< HEAD
library(matrixdist) A &amp;lt;- rmatrixnorm(100, mean=array(0,dim=c(3,4)), U = toeplitz(c(1,.8,.64)), V = rWishart(1,7,diag(4))[,,1]) MLmatrixnorm(A, row.variance=&amp;quot;AR(1)&amp;quot;) $mean [,1] [,2] [,3] [,4] [1,] 0.07425945 -0.1277509 -0.15602140 0.5687155 [2,] 0.09761592 -0.1253225 -0.10766017 0.</description>
=======
library(matrixdist) A &amp;lt;- rmatrixnorm(100, mean=array(0,dim=c(3,4)), U = toeplitz(c(1,.8,.64)), V = rWishart(1,7,diag(4))[,,1]) MLmatrixnorm(A, row.variance=&amp;quot;AR(1)&amp;quot;) $mean [,1] [,2] [,3] [,4] [1,] -0.14927218 0.14837747 -0.12314591 0.2203576 [2,] 0.03791425 0.00645731 -0.05735620 0.</description>
>>>>>>> b3070be22caa29c6eb32a0cce927d4c72d8e5762
    </item>
    
    <item>
      <title>Working with C and R</title>
      <link>http://gzt.github.io/post/working-with-c-and-r/</link>
      <pubDate>Tue, 13 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>http://gzt.github.io/post/working-with-c-and-r/</guid>
      <description>or: adventures with .Call Generally I work either in R or in C but not both. My research is trying to do some big things faster and there’s existing code in C that I’m working with. I might dump some output and then explore it in R, but I don’t need to interface between them. On the other hand, when consulting with others it’s going to come down to good old-fashioned statistics so that happens in R (or SAS if applicable, for some things it’s better and for some things the people you’re working with are using SAS).</description>
    </item>
    
    <item>
      <title>why I&#39;ve been using base plot</title>
      <link>http://gzt.github.io/post/why-i-ve-been-using-base-plot/</link>
      <pubDate>Tue, 13 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>http://gzt.github.io/post/why-i-ve-been-using-base-plot/</guid>
      <description>Nobody cares what I think, of course. The grammar of graphics is The Way, and ggplot2 is my preferred method of visualization. I never even really tried to make my own graphs with base plot until some point last fall (so: after 5 years of R) - I mean, I’m at Iowa State, after all. I don’t hang out with our Graphics Group as much as I should but that’s how I think about plotting.</description>
    </item>
    
    <item>
      <title>Some linear algebra tricks</title>
      <link>http://gzt.github.io/post/some-linear-algebra-tricks/</link>
      <pubDate>Mon, 12 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>http://gzt.github.io/post/some-linear-algebra-tricks/</guid>
      <description>You may have come across some of this before, but I wanted to write this down for some future reference because it came up in a project of mine and I want to refer to it later. The general theme is that if you know something about the structure of the matrices you’re working with, you can sometimes speed some things up.
Avoiding matrix inversion. Why do you want to invert your matrix?</description>
    </item>
    
  </channel>
</rss>